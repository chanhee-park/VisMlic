{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNfor MNIST Dataset\n",
    "\n",
    "### 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from skimage import io\n",
    "\n",
    "import theano\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Functions \n",
    "def get_hot_idx(arr):\n",
    "    return arr.index(max(arr))\n",
    "\n",
    "def save_to_json_file(data, filename):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    print(filename + ' 저장완료')\n",
    "    \n",
    "def get_round_array(array, decimal):\n",
    "    return [round(e, decimal) for e in array]\n",
    "\n",
    "def get_activations(model, layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations\n",
    "\n",
    "def get_arr_from_json_file(filename):\n",
    "    input_file = open (filename)\n",
    "    return json.load(input_file)\n",
    "    \n",
    "current_milli_time = lambda: int(round(time.time() * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 28\n",
    "image_height = 28\n",
    "num_of_features = image_width * image_height\n",
    "\n",
    "num_of_class = 10\n",
    "num_of_trainset = 60000\n",
    "num_of_testset = 10000\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(num_of_trainset, num_of_features).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(num_of_testset, num_of_features).astype('float32') / 255.0\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 테스트 데이터 로드\n",
    "images = np.zeros((10000, 784))\n",
    "for real in range(10):\n",
    "    for idx in range(1, 1001):\n",
    "        file = '../../data/mnist/images/'+ str(real) + '/' + str(real) + '_' + str(idx) +'.png'\n",
    "        image = np.ndarray.flatten(io.imread(file)) / 255.0\n",
    "        image = np.array([1 - pixel for pixel in image])\n",
    "        images[real * 1000 + idx - 1] = image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구축 \n",
    "model = Sequential()\n",
    "model.add(Dense(units=16, input_dim=num_of_features, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=num_of_class, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 1.3059 - acc: 0.5706\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.4436 - acc: 0.8763\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.3442 - acc: 0.9035: 0s - loss: 0.3529 \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2962 - acc: 0.9161\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2651 - acc: 0.9243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4a4a94b00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 셋 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 16us/step\n",
      "loss_and_metrics : [0.26745183908045295, 0.9279]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "pred_proba =  model.predict_proba(images).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9194, 'recall': [0.955, 0.96, 0.948, 0.933, 0.925, 0.82, 0.943, 0.934, 0.852, 0.924], 'precision': [0.9227, 0.9571, 0.8745, 0.8928, 0.9546, 0.9361, 0.9392, 0.9406, 0.9083, 0.8775]}\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인 및 저장\n",
    "correct = 0\n",
    "performances = {\n",
    "    \"accuracy\": 0,\n",
    "    \"recall\": [0] * 10,\n",
    "    \"precision\": [0] * 10\n",
    "}\n",
    "truePredict = [0] * 10\n",
    "numOfPredict = [0] * 10\n",
    "predicts = []\n",
    "\n",
    "for i in range(len(pred_proba)):\n",
    "    prob = pred_proba[i]\n",
    "    pred = get_hot_idx(prob)\n",
    "    real = i // 1000\n",
    "    predicts.append(dict({\n",
    "        \"real\": real,\n",
    "        \"pred\": pred,\n",
    "        \"prob\": [round(e, 4) for e in prob]\n",
    "    }))\n",
    "    numOfPredict[pred] = numOfPredict[pred] + 1\n",
    "    if pred is real:\n",
    "        truePredict[real] = truePredict[real] + 1\n",
    "        correct = correct + 1\n",
    "\n",
    "performances[\"accuracy\"] = correct / 10000\n",
    "performances[\"recall\"] = [round(truePredict[i] / 1000, 4) for i in range(10) ]\n",
    "performances[\"precision\"] = [round(truePredict[i] / numOfPredict[i], 4) for i in range(10)]\n",
    "\n",
    "print(performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원축소\n",
    "\n",
    "1) 마지막 직전의 레이어에서 activation values를 뽑는다.\n",
    "\n",
    "2) t-SNE로 차원을 축소한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "idxs = get_arr_from_json_file('./sample_image_idxs.json')\n",
    "for idx in idxs:\n",
    "    samples.append(images[idx])\n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = get_activations(model, -2, samples)[0]\n",
    "y = TSNE(n_components=2).fit_transform(extracted_features)\n",
    "sne_map = []\n",
    "for e in y:\n",
    "    sne_map.append({\n",
    "        \"x\": round(e[0], 2),\n",
    "        \"y\": round(e[1], 2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "    \"model_name\": 'Convolutional Neural Network',\n",
    "    \"short_name\": 'CNN',\n",
    "    \"description\": '1개의 Convolution2D 레이어와 1개의 Dense 레이어를 가지는 모델이다.',\n",
    "    \"performance\": {\n",
    "        \"accuracy\": performances[\"accuracy\"],\n",
    "        \"recall\": performances[\"recall\"],\n",
    "        \"precision\": performances[\"precision\"]\n",
    "    },\n",
    "    \"predict\": predicts,\n",
    "    \"t-sne\": sne_map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mnist_cnn.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(str(model_data), f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
