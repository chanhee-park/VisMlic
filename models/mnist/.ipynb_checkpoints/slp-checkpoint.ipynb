{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model for MNIST Dataset\n",
    "\n",
    "### 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from skimage import io\n",
    "\n",
    "import theano\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Functions \n",
    "def get_hot_idx(arr):\n",
    "    return arr.index(max(arr))\n",
    "\n",
    "def save_to_json_file(data, filename):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    print(filename + ' 저장완료')\n",
    "    \n",
    "def get_round_array(array, decimal):\n",
    "    return [round(e, decimal) for e in array]\n",
    "\n",
    "def get_activations(model, layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations\n",
    "\n",
    "def get_arr_from_json_file(filename):\n",
    "    input_file = open (filename)\n",
    "    return json.load(input_file)\n",
    "    \n",
    "current_milli_time = lambda: int(round(time.time() * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 28\n",
    "image_height = 28\n",
    "num_of_features = image_width * image_height\n",
    "\n",
    "num_of_class = 10\n",
    "num_of_trainset = 60000\n",
    "num_of_testset = 10000\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(num_of_trainset, num_of_features).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(num_of_testset, num_of_features).astype('float32') / 255.0\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 테스트 데이터 로드\n",
    "images = np.zeros((10000, 784))\n",
    "for real in range(10):\n",
    "    for idx in range(1, 1001):\n",
    "        file = '../../data/mnist/images/'+ str(real) + '/' + str(real) + '_' + str(idx) +'.png'\n",
    "        image = np.ndarray.flatten(io.imread(file)) / 255.0\n",
    "        image = np.array([1 - pixel for pixel in image])\n",
    "        images[real * 1000 + idx - 1] = image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 17:48:16.637328 4552037824 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0716 17:48:16.650506 4552037824 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0716 17:48:16.653007 4552037824 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0716 17:48:16.713616 4552037824 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0716 17:48:16.729713 4552037824 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 구축 \n",
    "model = Sequential()\n",
    "model.add(Dense(units=16, input_dim=num_of_features, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=num_of_class, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0716 17:48:16.823499 4552037824 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0716 17:48:16.875919 4552037824 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 1.4083 - acc: 0.5184\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.5157 - acc: 0.8501\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3757 - acc: 0.8941\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3236 - acc: 0.9094\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2902 - acc: 0.9190: 1s - loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb480f9de80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 셋 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 12us/step\n",
      "loss_and_metrics : [0.2738391203939915, 0.9188]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "pred_proba =  model.predict_proba(images).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.911, 'recall': [0.933, 0.974, 0.908, 0.833, 0.93, 0.852, 0.95, 0.91, 0.911, 0.909], 'precision': [0.9396, 0.9206, 0.8911, 0.9466, 0.9235, 0.9045, 0.8845, 0.9479, 0.8586, 0.9036]}\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인 및 저장\n",
    "correct = 0\n",
    "performances = {\n",
    "    \"accuracy\": 0,\n",
    "    \"recall\": [0] * 10,\n",
    "    \"precision\": [0] * 10\n",
    "}\n",
    "truePredict = [0] * 10\n",
    "numOfPredict = [0] * 10\n",
    "predicts = []\n",
    "\n",
    "for i in range(len(pred_proba)):\n",
    "    prob = pred_proba[i]\n",
    "    pred = get_hot_idx(prob)\n",
    "    real = i // 1000\n",
    "    predicts.append(dict({\n",
    "        \"real\": real,\n",
    "        \"pred\": pred,\n",
    "        \"prob\": [round(e, 4) for e in prob]\n",
    "    }))\n",
    "    numOfPredict[pred] = numOfPredict[pred] + 1\n",
    "    if pred is real:\n",
    "        truePredict[real] = truePredict[real] + 1\n",
    "        correct = correct + 1\n",
    "\n",
    "performances[\"accuracy\"] = correct / 10000\n",
    "performances[\"recall\"] = [round(truePredict[i] / 1000, 4) for i in range(10) ]\n",
    "performances[\"precision\"] = [round(truePredict[i] / numOfPredict[i], 4) for i in range(10)]\n",
    "\n",
    "print(performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원축소\n",
    "\n",
    "1) 마지막 직전의 레이어에서 activation values를 뽑는다.\n",
    "\n",
    "2) t-SNE로 차원을 축소한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "idxs = get_arr_from_json_file('./sample_image_idxs.json')\n",
    "for idx in idxs:\n",
    "    samples.append(images[idx])\n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = get_activations(model, -2, samples)[0]\n",
    "y = TSNE(n_components=2).fit_transform(extracted_features)\n",
    "sne_map = []\n",
    "for e in y:\n",
    "    sne_map.append({\n",
    "        \"x\": round(e[0], 2),\n",
    "        \"y\": round(e[1], 2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "    \"model_name\": 'Deep Neural Network',\n",
    "    \"short_name\": 'DNN',\n",
    "    \"description\": '5개의 히든 레이어를 가지는 심층 신경망 모델 (10-16-16-16-16-10)이다. epochs=5, batch_size=32',\n",
    "    \"performance\": {\n",
    "        \"accuracy\": performances[\"accuracy\"],\n",
    "        \"recall\": performances[\"recall\"],\n",
    "        \"precision\": performances[\"precision\"]\n",
    "    },\n",
    "    \"predict\": predicts,\n",
    "    \"t-sne\": sne_map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dnn_result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(str(model_data), f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
